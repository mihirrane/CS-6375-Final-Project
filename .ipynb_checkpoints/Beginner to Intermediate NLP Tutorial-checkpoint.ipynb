{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "434834c5a28bdcd4ace40545ce05177e08ac724e"
   },
   "source": [
    "Beginner to Intermediate Natural Language Processing Guide\n",
    "==================\n",
    "---\n",
    "## Table of Contents\n",
    "\n",
    "---\n",
    "- [**0.0 Setup**](#0.0-setup)\n",
    "    + [**0.1 Python & Anaconda**](#01-python-&-anaconda)\n",
    "    + [**0.2 Libraries**](#02-libraries)\n",
    "    + [**0.3 Other**](#03-other)\n",
    "- [**1.0 Background**](#10-background)\n",
    "    + [**1.1 What is NLP?**](#11-what-is-nlp)\n",
    "    + [**1.2 Why is NLP Important?**](#12-why-is-nlp-importance)\n",
    "    + [**1.3 Why is NLP a \"hard\" problem?**](#13-why-is-nlp-a-hard-problem)\n",
    "    + [**1.4 Glossary**](#14-glossary)\n",
    "- [**2.0 Sentiment Analysis**](#20-sentiment-analysis)\n",
    "    + [**2.1 Preparing the Data**](#21-preparing-the-data)\n",
    "        * [**2.1.1 Training Data**](#211-training-data)\n",
    "        * [**2.1.2 Test Data**](#212-test-data)\n",
    "    + [**2.2 Building a Classifier**](#22-building-a-classifier)\n",
    "    + [**2.3 Classification**](#53-classification)\n",
    "    + [**2.4 Accuracy**](#24-accuracy)\n",
    "- [**3.0 Regular Expressions**](#30-regular-expressions)\n",
    "    + [**3.1 Simplest Form**](#31-simplest-form)\n",
    "    + [**3.2 Case Sensitivity**](#32-case-sensitivity)\n",
    "    + [**3.3 Disjunctions**](#33-disjunctions) \n",
    "    + [**3.4 Ranges**](#34-ranges) \n",
    "    + [**3.5 Exclusions**](#35-exclusions) \n",
    "    + [**3.6 Question Marks**](#36-question-marks) \n",
    "    + [**3.7 Kleene Star**](#37-kleene-star) \n",
    "    + [**3.8 Wildcards**](#38-wildcards) \n",
    "    + [**3.9 Kleene+**](#39-kleene) \n",
    "- [**4.0 Word Tagging and Models**](#40-word-tagging-and-models)\n",
    "    + [**4.1 NLTK Parts of Speech Tagger**](#41-nltk-parts-of-speech-tagger)\n",
    "        * [**4.1.1 Ambiguity**](#411-ambiguity)\n",
    "    + [**4.2 Unigram Models**](#42-unigram-models)\n",
    "    + [**4.3 Bigram Models**](#43-bigram-models)\n",
    "- [**5.0 Normalizing Text**](#40-normalizing-text)\n",
    "    + [**5.1 Stemming**](#51-stemming)\n",
    "        * [**5.1.1 What is Stemming?**](#511-what-is-stemming)\n",
    "        * [**5.1.2 Types of Stemmers**](#512-types-of-stemmers)\n",
    "    + [**5.2 Lemmatization**](#52-lemmatization)\n",
    "        * [**5.2.1 What is Lemmatization?**](#521-what-is-lemmatization)\n",
    "        * [**5.2.2 WordNetLemmatizer?**](#522-wordnetlemmatizer)\n",
    "- [**6.0 Final Words**](#60-final-words)\n",
    "    + [**6.1 Resources**](#61-resources)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d1528538b54a13e5090a0ae5baf6e51b7b830a04"
   },
   "source": [
    "## 0.0 Setup\n",
    "---\n",
    "\n",
    "This NLP guide was written in Python 3.6.\n",
    "\n",
    "### 0.1 Python & Anaconda\n",
    "\n",
    "Download [Python](https://www.python.org/downloads/) and [Pip](http://docs.continuum.io/anaconda/install).\n",
    "\n",
    "\n",
    "### 0.2 Libraries\n",
    "\n",
    "* Here, We are working with `re` library for **regular expressions** and `nltk` for **natural language processing** techniques, so make sure to install them! To install these libraries, enter the following commands into your terminal: \n",
    "\n",
    "``` \n",
    "pip3 install re\n",
    "pip3 install nltk\n",
    "```\n",
    "\n",
    "### 0.3 Other\n",
    "\n",
    "Since we'll be working on textual analysis, we'll be using datasets that are already well established and widely used. To gain access to these datasets, enter the following command into your command line: (Note that this might take a few minutes!)\n",
    "\n",
    "```\n",
    "sudo python3 -m nltk.downloader all\n",
    "```\n",
    "\n",
    "Lastly, download the data we'll be working with in this example! \n",
    "\n",
    "[Positive Tweets](https://github.com/lesley2958/natural-language-processing/blob/master/pos_tweets.txt) <br>\n",
    "[Negative Tweets](https://github.com/lesley2958/natural-language-processing/blob/master/neg_tweets.txt)\n",
    "\n",
    "Now you're all set to begin!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "source": [
    "## 1.0 Background\n",
    "---\n",
    "\n",
    "### 1.1 What is NLP? \n",
    "\n",
    "Natural Language Processing, or NLP, is an area of computer science that focuses on developing techniques to produce machine-driven analyses of text.[**...Wikipedia**](https://www.google.co.in/url?sa=t&rct=j&q=&esrc=s&source=web&cd=29&cad=rja&uact=8&ved=2ahUKEwiakISXsoHfAhXUXysKHYQpAHIQmhMwHHoECAMQAg&url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FNatural_language_processing&usg=AOvVaw1KDOCNufwMWaoNt_JWX4QL)\n",
    "\n",
    "### 1.2 Why is Natural Language Processing Important? \n",
    "\n",
    "* NLP expands the **unmitigated amount of data that can be used for getting insight for use**. Since so much of the data we have available is in the **major format of text,** this is exceedingly important to data science and for Industry !\n",
    "\n",
    "* A specific common application of **NLP** is each time you use a **language conversion tool.** This techniques used to **accurately convert text from one language to another**(Like **Google Translator**) very much falls under the umbrella of **\"natural language processing.\"**\n",
    "\n",
    "### 1.3 Why is NLP a \"hard\" problem? \n",
    "\n",
    "* The language is ambiguous. Once, the explanation of a person's sentence can be very different from another person's explanation. Because of this inability to be constantly clear, it is difficult to have a NLP technique that works perfectly.\n",
    "\n",
    "### 1.4 Glossary\n",
    "\n",
    "Here is some common terminology:\n",
    "\n",
    "<b>Corpus: </b> (Plural: Corpora) a collection of written texts that serve as our datasets.\n",
    "\n",
    "<b>nltk: </b> (Natural Language Toolkit) the python module we'll be using repeatedly; it has a lot of useful built-in NLP techniques.\n",
    "\n",
    "<b>Token: </b> a string of contiguous characters between two spaces, or between a space and punctuation marks. A token can also be an integer, real, or a number with a colon.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0423dc4a4511265f8d23d2e72af138f776ae0146"
   },
   "source": [
    "## 2.0 Sentiment Analysis  \n",
    "---\n",
    "\n",
    "So you are wondering, what exactly is \"the Sentimental Analysis\"? \n",
    "\n",
    "Well, sentiment analysis involves building a system to collect and determine the emotional tone behind the words. This is important because you can understand the attitudes, opinions and emotions of people in your data. \n",
    "\n",
    "At a high level, sentiment analysis involves the processing of **natural language and artificial intelligence** by assuming that the actual element of the text is converted to a format that can be read by a machine, and statistics are used to determine the actual sentiment,\n",
    "\n",
    "### 2.1 Preparing the Data \n",
    "\n",
    "To accomplish sentiment analysis computationally, we have to use techniques that will allow us to learn from data that's already been labeled. \n",
    "\n",
    "So what's the first step? Formatting the data so that we can actually apply NLP techniques. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "f8e90abefb5b156fa37b59800fb1a405b6046e18"
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\ranem/nltk_data'\n    - 'C:\\\\Users\\\\ranem\\\\Anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\ranem\\\\Anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\ranem\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\ranem\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-480846918241>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mTrue\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mformat_sentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Life is beautiful so Enjoy everymoment you have.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1-480846918241>\u001b[0m in \u001b[0;36mformat_sentence\u001b[1;34m(sent)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mformat_sentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[1;32mreturn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mTrue\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mformat_sentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Life is beautiful so Enjoy everymoment you have.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m     \"\"\"\n\u001b[1;32m--> 144\u001b[1;33m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m     return [\n\u001b[0;32m    146\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     \"\"\"\n\u001b[1;32m--> 105\u001b[1;33m     \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    866\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    867\u001b[0m     \u001b[1;31m# Load the resource.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 868\u001b[1;33m     \u001b[0mopened_resource\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    869\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'raw'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    991\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    992\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'nltk'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 993\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    994\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'file'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    995\u001b[0m         \u001b[1;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    699\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'*'\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    700\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 701\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    702\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\ranem/nltk_data'\n    - 'C:\\\\Users\\\\ranem\\\\Anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\ranem\\\\Anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\ranem\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\ranem\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "def format_sentence(sent):\n",
    "    return({word: True for word in nltk.word_tokenize(sent)})\n",
    "\n",
    "format_sentence(\"Life is beautiful so Enjoy everymoment you have.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ad8066f3cee0d645435b66e1dd45ae4e4c70c5a6"
   },
   "source": [
    "Here, `format_sentence` changes a piece of text, in this case a tweet, into a dictionary of words mapped to True booleans. Though not obvious from this function alone, this will eventually allow us to train  our prediction model by splitting the text into its tokens, i.e. <i>tokenizing</i> the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6f239b37d0be47bd6d7c3d832bb35ab411be6bdf"
   },
   "outputs": [],
   "source": [
    "pos = []\n",
    "with open(\"../input/sentimental-analysis-nlp/pos_tweets.txt\") as f:\n",
    "    for i in f: \n",
    "        pos.append([format_sentence(i), 'pos'])\n",
    "        \n",
    "pos[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9e96f0dd551eb8919da668227de5dcd161148ef7"
   },
   "outputs": [],
   "source": [
    "neg = []\n",
    "with open(\"../input/sentimental-analysis-nlp/neg_tweets.txt\") as f:\n",
    "    for i in f: \n",
    "        neg.append([format_sentence(i), 'neg'])\n",
    "        \n",
    "neg[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "afa04d65a38912bd7cf2c8ad1f2d0ff95864507a"
   },
   "source": [
    "#### 2.1.1 Training Data\n",
    "\n",
    "Next, we'll split the labeled data we have into two pieces, one that can \"train\" data and the other to give us insight on how well our model is performing. The training data will inform our model on which features are most important.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b0bd6dd376a2487a0d1a5e919b6dee5662c5dbc9"
   },
   "outputs": [],
   "source": [
    "training = pos[:int((.9)*len(pos))] + neg[:int((.9)*len(neg))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d6c9e067b7d1ccdcf8cb3e56fe2d8c4ace835eb0"
   },
   "source": [
    "#### 2.1.2 Test Data\n",
    "\n",
    "We won't use the test data until the very end of this section, but nevertheless, we save the last 10% of the data to check the accuracy of our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6d57bd0aa79210522df7e2c6cc3b0ac7df856d44"
   },
   "outputs": [],
   "source": [
    "test = pos[int((.1)*len(pos)):] + neg[int((.1)*len(neg)):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "dae4ac7594400aaae31a75cb994fddc15725e0f2"
   },
   "source": [
    "### 2.2 Building a Classifier\n",
    "\n",
    "All NLTK classifiers work with feature structures, which can be simple dictionaries mapping a feature name to a feature value. In this example, we’ve used a simple bag of words model where every word is a feature name with a value of True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b26e66e3fc07fbeba07ea354ad40d9664efefa94"
   },
   "outputs": [],
   "source": [
    "from nltk.classify import NaiveBayesClassifier\n",
    "\n",
    "classifier = NaiveBayesClassifier.train(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b8b923adef57762b51818a9732c34b8cf12743d9"
   },
   "source": [
    "To see which features informed our model the most, we can run this line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "09cd55068df0f43b1cbefe1480ecef2b82a16e74"
   },
   "outputs": [],
   "source": [
    "classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f74e4f7a5608038dadabefc632d5fbd2cde62f54"
   },
   "source": [
    "### 2.3 Classification\n",
    "\n",
    "Just to see that our model works, let's try the classifier out with a positive example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8e14183b19604e659182ab5c88e1b5506a42e356"
   },
   "outputs": [],
   "source": [
    "example1 = \"this workshop is awesome.\"\n",
    "example2 = \"This workshop is not good\"\n",
    "\n",
    "print(classifier.classify(format_sentence(example1)))\n",
    "print(classifier.classify(format_sentence(example2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ad31be3d133b715cd3c66ea43cfdec5c8177a846"
   },
   "source": [
    "### 2.4 Accuracy\n",
    "\n",
    "Now, there's no point in building a model if it doesn't work well. Luckily, once again, nltk comes to the rescue with a built in feature that allows us find the accuracy of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b1c081504c628375a862cc46753a8ec79c0fe499"
   },
   "outputs": [],
   "source": [
    "from nltk.classify.util import accuracy\n",
    "\n",
    "print(accuracy(classifier, test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9b42be7f17ec33bf0a2d81a39ef55ca7b1d40d1c"
   },
   "source": [
    "Turns out it works decently well!\n",
    "\n",
    "But it could be better! I think we can agree that the data is kind of messy - there are typos, abbreviations, grammatical errors of all sorts... So how do we handle that? Can we handle that? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3e893b8bd975b4a8da5f1f547509c828d44c4876"
   },
   "source": [
    "## 3.0 Regular Expressions\n",
    "---\n",
    "\n",
    "A regular expression is a sequence of characters that define a string.\n",
    "\n",
    "### 3.1 Simplest Form\n",
    "\n",
    "The simplest form of a regular expression is a sequence of characters contained within <b>two backslashes</b>. For example, <i>python</i> would be  \n",
    "\n",
    "``` \n",
    "\\python\n",
    "```\n",
    "\n",
    "### 3.2 Case Sensitivity\n",
    "\n",
    "Regular Expressions are <b>case sensitive</b>, which means \n",
    "\n",
    "``` \n",
    "\\p and \\P\n",
    "```\n",
    "are distinguishable from eachother. This means <i>python</i> and <i>Python</i> would have to be represented differently, as follows: \n",
    "\n",
    "``` \n",
    "\\python and \\Python\n",
    "```\n",
    "\n",
    "We can check these are different by running:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5345585c5047513a8e6720df90d3277ce14d6c48"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "re1 = re.compile('python')\n",
    "print(bool(re1.match('Python')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9041ad546d3fe295c0100b7515c3c2c7d782ddb0"
   },
   "source": [
    "\n",
    "### 3.3 Disjunctions\n",
    "\n",
    "If you want a regular expression to represent both <i>python</i> and <i>Python</i>, however, you can use <b>brackets</b> or the <b>pipe</b> symbol as the disjunction of the two forms. For example, \n",
    "\n",
    "``` \n",
    "[Pp]ython or \\Python|python\n",
    "```\n",
    "\n",
    "could represent either <i>python</i> or <i>Python</i>. Likewise, \n",
    "\n",
    "``` \n",
    "[0123456789]\n",
    "```\n",
    "\n",
    "would represent a single integer digit. The pipe symbols are typically used for interchangable strings, such as in the following example:\n",
    "\n",
    "```\n",
    "\\dog|cat\n",
    "```\n",
    "\n",
    "### 3.4 Ranges\n",
    "\n",
    "If we want a regular expression to express the disjunction of a range of characters, we can use a <b>dash</b>. For example, instead of the previous example, we can write \n",
    "\n",
    "``` \n",
    "[0-9]\n",
    "```\n",
    "Similarly, we can represent all characters of the alphabet with \n",
    "\n",
    "``` \n",
    "[a-z]\n",
    "```\n",
    "\n",
    "### 3.5 Exclusions\n",
    "\n",
    "Brackets can also be used to represent what an expression <b>cannot</b> be if you combine it with the <b>caret</b> sign. For example, the expression \n",
    "\n",
    "``` \n",
    "[^p]\n",
    "```\n",
    "represents any character, special characters included, but p.\n",
    "\n",
    "### 3.6 Question Marks \n",
    "\n",
    "Question marks can be used to represent the expressions containing zero or one instances of the previous character. For example, \n",
    "\n",
    "``` \n",
    "<i>\\colou?r\n",
    "```\n",
    "represents either <i>color</i> or <i>colour</i>. Question marks are often used in cases of plurality. For example, \n",
    "\n",
    "``` \n",
    "<i>\\computers?\n",
    "```\n",
    "can be either <i>computers</i> or <i>computer</i>. If you want to extend this to more than one character, you can put the simple sequence within parenthesis, like this:\n",
    "\n",
    "```\n",
    "\\Feb(ruary)?\n",
    "```\n",
    "This would evaluate to either <i>February</i> or <i>Feb</i>.\n",
    "\n",
    "### 3.7 Kleene Star\n",
    "\n",
    "To represent the expressions containing zero or <b>more</b> instances of the previous character, we use an <b>asterisk</b> as the kleene star. To represent the set of strings containing <i>a, ab, abb, abbb, ...</i>, the following regular expression would be used:  \n",
    "```\n",
    "\\ab*\n",
    "```\n",
    "\n",
    "### 3.8 Wildcards\n",
    "\n",
    "Wildcards are used to represent the possibility of any character and symbolized with a <b>period</b>. For example, \n",
    "\n",
    "```\n",
    "\\beg.n\n",
    "```\n",
    "From this regular expression, the strings <i>begun, begin, began,</i> etc., can be generated. \n",
    "\n",
    "### 3.9 Kleene+\n",
    "\n",
    "To represent the expressions containing at <b>least</b> one or more instances of the previous character, we use a <b>plus</b> sign. To represent the set of strings containing <i>ab, abb, abbb, ...</i>, the following regular expression would be used:  \n",
    "\n",
    "```\n",
    "\\ab+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f576cfa4118f295af0cd0df5c9382f88b0701409"
   },
   "source": [
    "## 4.0 Word Tagging and Models\n",
    "---\n",
    "\n",
    "Any phrase, you can classify each word as a noun, a verb, a conjunction or any other type of words. When there are hundreds of thousands of prayers, even millions, it is obviously a huge and boring task. But it's not an impossible problem to solve on the IT front.\n",
    "\n",
    "### 4.1 NLTK Parts of Speech Tagger\n",
    "\n",
    "NLTK is a Python package that provides libraries for various word processing techniques, e.g. Classification, tokenization, stemming, parsing, but important for this example, tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "bb425bc683a7daeb4a9e15f10bcd9854bc212ad4"
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\ranem/nltk_data'\n    - 'C:\\\\Users\\\\ranem\\\\Anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\ranem\\\\Anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\ranem\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\ranem\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-bbd44ce41f1a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Python is an awesome language!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m     \"\"\"\n\u001b[1;32m--> 144\u001b[1;33m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m     return [\n\u001b[0;32m    146\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     \"\"\"\n\u001b[1;32m--> 105\u001b[1;33m     \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    866\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    867\u001b[0m     \u001b[1;31m# Load the resource.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 868\u001b[1;33m     \u001b[0mopened_resource\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    869\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'raw'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    991\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    992\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'nltk'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 993\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    994\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'file'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    995\u001b[0m         \u001b[1;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    699\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'*'\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    700\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 701\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    702\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\ranem/nltk_data'\n    - 'C:\\\\Users\\\\ranem\\\\Anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\ranem\\\\Anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\ranem\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\ranem\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "\n",
    "text = nltk.word_tokenize(\"Python is an awesome language!\")\n",
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ac3019ff568692f4207cc53d67d679582a6290ab"
   },
   "source": [
    "Not sure what DT, JJ, or any other tag is? Just try this in your python shell: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "18c277e10ea4b30d144f7dcd8afc4b7d4438edb2"
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mtagsets\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('tagsets')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mhelp/tagsets/upenn_tagset.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\ranem/nltk_data'\n    - 'C:\\\\Users\\\\ranem\\\\Anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\ranem\\\\Anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\ranem\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\ranem\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-75aa3c6606a2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhelp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupenn_tagset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'JJ'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\help.py\u001b[0m in \u001b[0;36mupenn_tagset\u001b[1;34m(tagpattern)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mupenn_tagset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtagpattern\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[0m_format_tagset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"upenn_tagset\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtagpattern\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\help.py\u001b[0m in \u001b[0;36m_format_tagset\u001b[1;34m(tagset, tagpattern)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_format_tagset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtagset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtagpattern\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m     \u001b[0mtagdict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"help/tagsets/\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtagset\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\".pickle\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtagpattern\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[0m_print_entries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtagdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtagdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    866\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    867\u001b[0m     \u001b[1;31m# Load the resource.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 868\u001b[1;33m     \u001b[0mopened_resource\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    869\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'raw'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    991\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    992\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'nltk'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 993\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    994\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'file'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    995\u001b[0m         \u001b[1;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    699\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'*'\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    700\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 701\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    702\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mtagsets\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('tagsets')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mhelp/tagsets/upenn_tagset.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\ranem/nltk_data'\n    - 'C:\\\\Users\\\\ranem\\\\Anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\ranem\\\\Anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\ranem\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\ranem\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset('JJ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e07d8cfea0fb20adabcc0fbce99153e4f9446dea"
   },
   "source": [
    "#### 4.1.1 Ambiguity\n",
    "\n",
    "But what happens if a word can be described as more than a part of speech? For example, the word \"sink\". Depending on the content of the sentence, it may be a noun or a verb.\n",
    "\n",
    "What happens when a text is a rhetorical instrument like sarcasm or irony? Of course, this can mislead the sentiment analyzer to misclassify a regular expression.\n",
    "\n",
    "### 4.2 Unigram Models\n",
    "\n",
    "Do you remember our word bag model from earlier? One of the features was that the word order was not taken into account. Therefore, the dictionaries can be used to map each word into true values.\n",
    "\n",
    "Unigram models are models where the order of our model makes no difference. You may be wondering why the unigram models interest us because they seem so simple without leaving their simplicity. They are a fundamental building block for many advanced NLP techniques.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "d598484053f420ec94c9f750080fb22d03d8e05d"
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mbrown\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('brown')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/brown\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\ranem/nltk_data'\n    - 'C:\\\\Users\\\\ranem\\\\Anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\ranem\\\\Anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\ranem\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\ranem\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     85\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m                     \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{}/{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    700\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 701\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    702\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mbrown\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('brown')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/brown.zip/brown/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\ranem/nltk_data'\n    - 'C:\\\\Users\\\\ranem\\\\Anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\ranem\\\\Anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\ranem\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\ranem\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-a2af02ab84a8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbrown\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mbrown_tagged_sents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbrown\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtagged_sents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcategories\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'news'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mbrown_sents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbrown\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcategories\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'news'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0munigram_tagger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUnigramTagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbrown_tagged_sents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m         \u001b[1;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[1;31m# __class__ to something new:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     86\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{}/{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[1;31m# Load the corpus.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m                 \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{}/{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    699\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'*'\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    700\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 701\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    702\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mbrown\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('brown')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/brown\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\ranem/nltk_data'\n    - 'C:\\\\Users\\\\ranem\\\\Anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\ranem\\\\Anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\ranem\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\ranem\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "\n",
    "brown_tagged_sents = brown.tagged_sents(categories='news')\n",
    "brown_sents = brown.sents(categories='news')\n",
    "unigram_tagger = nltk.UnigramTagger(brown_tagged_sents)\n",
    "unigram_tagger.tag(brown_sents[2007])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "11b03321be04121915287ebc3b7c1333a1d021f3"
   },
   "source": [
    "### 4.3 Bigram Models\n",
    "\n",
    "Here, ordering does matter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "db8ffa0efc27f5d403bfebb1f5be8aad5a5adff3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Various', 'JJ'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'AT'),\n",
       " ('apartments', 'NNS'),\n",
       " ('are', 'BER'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'AT'),\n",
       " ('terrace', 'NN'),\n",
       " ('type', 'NN'),\n",
       " (',', ','),\n",
       " ('being', 'BEG'),\n",
       " ('on', 'IN'),\n",
       " ('the', 'AT'),\n",
       " ('ground', 'NN'),\n",
       " ('floor', 'NN'),\n",
       " ('so', 'CS'),\n",
       " ('that', 'CS'),\n",
       " ('entrance', 'NN'),\n",
       " ('is', 'BEZ'),\n",
       " ('direct', 'JJ'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_tagger = nltk.BigramTagger(brown_tagged_sents)\n",
    "bigram_tagger.tag(brown_sents[2007])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ab8f5dc71d0c893e0eb066a404fae986f038827f"
   },
   "source": [
    "Note the changes since the last time we tagged the words of this same sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b6b4713aab46e926703c28aeac61422b43a21854"
   },
   "source": [
    "## 5.0 Normalizing Text\n",
    "---\n",
    "The best data is consistent data, textual data usually not. But we can do it by normalizing it. We can do a number of things for that.\n",
    "\n",
    "At least we can do all the text, so all lowercase. You may have done this before:\n",
    "\n",
    "Given a piece of text,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "5958dfdd22c8ffdcb6d8b4398d4300dff219feb4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['omg',\n",
       " ',',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'is',\n",
       " 'so',\n",
       " 'cool',\n",
       " 'and',\n",
       " 'i',\n",
       " \"'m\",\n",
       " 'really',\n",
       " 'enjoying',\n",
       " 'this',\n",
       " 'workshop',\n",
       " '!']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw = \"OMG, Natural Language Processing is SO cool and I'm really enjoying this workshop!\"\n",
    "tokens = nltk.word_tokenize(raw)\n",
    "tokens = [i.lower() for i in tokens]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "99be0a8969d8d6751c0ac441823f7d619cabc965"
   },
   "source": [
    "### 5.1 Stemming\n",
    "\n",
    "But we can do more! \n",
    "\n",
    "#### 5.1.1 What is Stemming?\n",
    "\n",
    "Stemming  is the process of converting the words of a sentence into non-editable parts. IIn the example of amusing, amusement, and amused above, the stem would be amus.\n",
    "\n",
    "#### 5.1.2 Types of Stemmers\n",
    "\n",
    "\n",
    "You're probably wondering how to convert a bunch of words into their **stems**(***similar meaning words into same words***). Fortunately, NLTK has some integrated and well-established stemmers devices you can use! They work a little differently because they follow different rules - which you use depends on what you are currently working on.\n",
    "\n",
    "Let's start with Lancaster Stemmer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "b44e01cd68c0805e51de51417da62d5897832c9c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['omg',\n",
       " ',',\n",
       " 'nat',\n",
       " 'langu',\n",
       " 'process',\n",
       " 'is',\n",
       " 'so',\n",
       " 'cool',\n",
       " 'and',\n",
       " 'i',\n",
       " \"'m\",\n",
       " 'real',\n",
       " 'enjoy',\n",
       " 'thi',\n",
       " 'workshop',\n",
       " '!']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lancaster = nltk.LancasterStemmer()\n",
    "stems = [lancaster.stem(i) for i in tokens]\n",
    "stems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fadc619741b27f79d259ba9963d19bf13b9b831e"
   },
   "source": [
    "Secondly, we try the Porter Stemmer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "514a1081c2ac952d4048a20681efaeb19036e7eb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['omg',\n",
       " ',',\n",
       " 'natur',\n",
       " 'languag',\n",
       " 'process',\n",
       " 'is',\n",
       " 'so',\n",
       " 'cool',\n",
       " 'and',\n",
       " 'i',\n",
       " \"'m\",\n",
       " 'realli',\n",
       " 'enjoy',\n",
       " 'thi',\n",
       " 'workshop',\n",
       " '!']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "stem = [porter.stem(i) for i in tokens]\n",
    "stem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b7f52d0d2341011fbacf62354068a9ddb2e2d903"
   },
   "source": [
    "Notice how \"natural\" maps to \"natur\" instead of \"nat\" and \"really\" maps to \"realli\" instead of \"real\" in the last stemmer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "056e6a2d33ab085e2df79acb7a583de1ea110201"
   },
   "source": [
    "### 5.2 Lemmatization\n",
    "\n",
    "#### 5.2.1 What is Lemmatization?\n",
    "\n",
    "Lemmatization is the process of converting words from a sentence into a dictionary.  For example, given the words amusement, amusing, and amused, the lemma for each and all would be amuse.\n",
    "\n",
    "#### 5.2.2 WordNetLemmatizer\n",
    "\n",
    "Once Again, NLTK is great and has an integrated Lemmatizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "7d50df04958dc7837d1839ff91218aceae721967"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['woman', 'in', 'technology', 'are', 'amazing', 'at', 'coding']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import WordNetLemmatizer\n",
    "\n",
    "lemma = nltk.WordNetLemmatizer()\n",
    "text = \"Women in technology are amazing at coding\"\n",
    "ex = [i.lower() for i in text.split()]\n",
    "lemmas = [lemma.lemmatize(i) for i in ex]\n",
    "lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "021e07cffa6d2f3e6f1f37a902bf4cddebd9c940"
   },
   "source": [
    "Notice that women is changed to \"woman\"! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a22603d8a240f1e0e6ee619800b37df0cb42847c"
   },
   "source": [
    "## 6.0 Final Words \n",
    "---\n",
    "\n",
    "Back to our first sentiment analysis, we could have improved our model in many ways by applying some of the techniques we have just experienced. The data from Twitter are seemingly chaotic and inconsistent. If we really wanted an extremely accurate model, we could have pre-processed the tweets to clean it up.\n",
    "\n",
    "Second, the way we built our classifier could have been improved. Our feature extraction was relatively simple and could have been improved with a bigram model instead of the word-bag template. We could have corrected our Bayes classifier to take only the most common words into account.\n",
    "\n",
    "### 6.1 Resources\n",
    "\n",
    "[Natural Language Processing With Python](http://bit.ly/nlp-w-python) <br>\n",
    "[Regular Expressions Cookbook](http://bit.ly/regular-expressions-cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e1a3cc4a9cbcd5b79f4ab4a17408596b964e8332"
   },
   "source": [
    "Intermediate Natural Language Processing\n",
    "==================\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "- [0.0 Setup](#00)\n",
    "    + [0.1 Python and Pip](#01)\n",
    "    + [0.2 Libraries](#02)\n",
    "    + [0.3 Other](#03)\n",
    "    + [0.4 Virtual Environment](#04)\n",
    "- [1.0 Background](#10)\n",
    "+ [1.1 Polarity Flippers](#11)\n",
    "    * [1.1.1 Negation](#111)\n",
    "+ [1.2 Multiword Expressions](#12)\n",
    "+ [1.3 WordNet](#13)\n",
    "    * [1.3.1 Synsets](#131)\n",
    "    * [1.3.2 Negation](#132)\n",
    "+ [1.4 SentiWordNet](#14)\n",
    "+ [1.5 Stop Words](#15)\n",
    "+ [1.6 Testing](#16)\n",
    "    * [1.6.1 Cross Validation](#161)\n",
    "    * [1.6.2 Precision](#162)\n",
    "+ [1.7 Logistic Regression](#17)\n",
    "- [2.0 Information Extraction](#20)\n",
    "+ [2.1 Data Forms](#21)\n",
    "+ [2.2 What is Information Extraction?](#22)\n",
    "- [3.0 Chunking](#30)\n",
    " + [3.1 Noun Phrase Chunking](#31)\n",
    "- [4.0 Named Entity Extraction](#40)\n",
    "    + [4.1 spaCy](#41)\n",
    "    + [4.2 nltk](#42)\n",
    "- [5.0 Relation Extraction](#50)\n",
    "    + [5.1 Rule-Based Systems](#51)\n",
    "    + [5.2 Machine Learning](#52)\n",
    "- [6.0 Sentiment Analysis](#60)\n",
    "    + [6.1 Loading the Data](#61)\n",
    "    + [6.2 Preparing the Data](#62)\n",
    "    + [6.3 Linear Classifier](#63)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "33172683d7c66ff43a05ce18d008f06bba5013d8"
   },
   "source": [
    "## 0.0 Setup <a id=\"00\"></a>\n",
    "\n",
    "---\n",
    "This guide was written in Python 3.6.\n",
    "\n",
    "### 0.1 Python & Pip <a id=\"01\"></a>\n",
    "\n",
    "If you haven't already, please download [Python](https://www.python.org/downloads/) and [Pip](https://pip.pypa.io/en/stable/installing/).\n",
    "\n",
    "### 0.2 Libraries <a id=\"02\"></a>\n",
    "\n",
    "We'll be working with the re library for regular expressions and nltk for natural language processing techniques, so make sure to install them! To install these libraries, enter the following commands into your terminal: \n",
    "\n",
    "``` \n",
    "pip3 install nltk==3.2.4\n",
    "pip3 install spacy==1.8.2\n",
    "pip3 install pandas==0.20.1\n",
    "pip3 install scikit-learn==0.18.1\n",
    "```\n",
    "\n",
    "### 0.3 Other <a id=\"03\"></a>\n",
    "\n",
    "Sentence boundary detection requires the dependency parse, which requires data to be installed, so enter the following command in your terminal. \n",
    "\n",
    "```\n",
    "python3 -m spacy.en.download all\n",
    "```\n",
    "\n",
    "### 0.4 Virtual Environment <a id=\"04\"></a>\n",
    "\n",
    "If you'd like to work in a virtual environment, you can set it up as follows: \n",
    "```\n",
    "pip3 install virtualenv\n",
    "virtualenv your_env\n",
    "```\n",
    "And then launch it with: \n",
    "```\n",
    "source your_env/bin/activate\n",
    "```\n",
    "\n",
    "To execute the visualizations in matplotlib, do the following:\n",
    "\n",
    "```\n",
    "cd ~/.matplotlib\n",
    "vim matplotlibrc\n",
    "```\n",
    "And then, write `backend: TkAgg` in the file. Now you should be set up with your virtual environment!\n",
    "\n",
    "Cool, now we're ready to start! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d0df80c5c9769761215b3843674861c42ab3b74f"
   },
   "source": [
    "## 1.0 Background <a id=\"10\"></a>\n",
    "\n",
    "---\n",
    "\n",
    "### 1.1 Polarity Flippers<a id=\"11\"></a>\n",
    "\n",
    "\n",
    "Polarity flippers are words that change positive expressions into negative ones or vice versa. \n",
    "\n",
    "#### 1.1.1 Negation <a id=\"111\"></a>\n",
    "\n",
    "Negations directly change an expression's sentiment by preceding the word before it. An example would be\n",
    "\n",
    "```\n",
    "The cat is not nice.\n",
    "```\n",
    "\n",
    "#### 1.1.2 Constructive Discourse Connectives\n",
    "\n",
    "Constructive Discourse Connectives are words which indirectly change an expression's meaning with words like \"but\". An example would be \n",
    "\n",
    "``` \n",
    "I usually like cats, but this cat is evil.\n",
    "```\n",
    "\n",
    "### 1.2 Multiword Expressions<a id=\"12\"></a>\n",
    "\n",
    "Multiword expressions are important because, depending on the context, can be considered positive or negative. For example, \n",
    "\n",
    "``` \n",
    "This song is shit.\n",
    "```\n",
    "is definitely considered negative. Whereas\n",
    "\n",
    "``` \n",
    "This song is the shit.\n",
    "```\n",
    "is actually considered positive, simply because of the addition of 'the' before the word 'shit'.\n",
    "\n",
    "### 1.3 WordNet<a id=\"13\"></a>\n",
    "\n",
    "WordNet is an English lexical database with emphasis on synonymy - sort of like a thesaurus. Specifically, nouns, verbs, adjectives and adjectives are grouped into synonym sets. \n",
    "\n",
    "#### 1.3.1 Synsets<a id=\"131\"></a>\n",
    "\n",
    "nltk has a built-in WordNet that we can use to find synonyms. We import it as such:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "20ea121cec1fbbec8e22a46957cc3b6710fb5c1b"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fa21f53abb41759e031a49bed26013d101b8658e"
   },
   "source": [
    "If we feed a word to the **synsets() method**, the ***return value will be the class to which belongs***. For example, ***if we call the method on motorcycle,*** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "56e385a3876b1fabde08bfca1545398a25975fa0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('car.n.01')]\n"
     ]
    }
   ],
   "source": [
    "print(wn.synsets('motorcar'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fb14ddc18c19daaa34f65efa248a723759a0a4ca"
   },
   "source": [
    "***Awesome stuff!*** But if we want to take it a step further, we can. We've previously learned what **lemmas are** - if you want to obtain the lemmas for a given **synonym set**, you can use the following method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "1feb0503f0bc16babcd65b6ae938a527d86e60b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['car', 'auto', 'automobile', 'machine', 'motorcar']\n"
     ]
    }
   ],
   "source": [
    "print(wn.synset('car.n.01').lemma_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "310b425a022cf69cbd44625ffdc96f16b7fcb4f4"
   },
   "source": [
    "Even more, you can do things like get the definition of a word: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "fb407c83da973a355af6788e5004a74372ed6d9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a motor vehicle with four wheels; usually propelled by an internal combustion engine\n"
     ]
    }
   ],
   "source": [
    "print(wn.synset('car.n.01').definition())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "17d124d6cafbf1abf547617b9bcecbdcfc93cb5c"
   },
   "source": [
    "#### 1.3.2 Negation<a id=\"132\"></a>\n",
    "\n",
    "With **WordNet,** we can easily **detect negations**. This is great because ***it's not only fast, but it requires no training data and has a fairly good predictive accuracy.*** On the other hand, it's not able to handle context well or work with multiple word phrases. \n",
    "\n",
    "\n",
    "### 1.4 SentiWordNet<a id=\"14\"></a>\n",
    "\n",
    "Based on **WordNet synsets, SentiWordNet is a lexical resource for opinion mining**, where ***each synset is assigned three sentiment scores: positivity, negativity, and objectivity.***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "c1fed37a6a666348cdd63b3336c5badb464181a7"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import sentiwordnet as swn\n",
    "cat = swn.senti_synset('cat.n.03')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "0717fae123a2f1467d4d694d192e9d1ad6950a8e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat.pos_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "dd123f77a819402e458c72a4382124e8f2bd3270"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.125"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat.neg_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "8488256b3a48a970918c11f039c520b86d6938b8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.875"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat.obj_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "bc9113c401a79d15c0f340218e308cb37df65547"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"SentiSynset('cat.n.03')\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat.unicode_repr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "10c1767b8216d9d6bfbf1246c15c82d952fb49bb"
   },
   "source": [
    "### 1.5 Stop Words<a id=\"15\"></a>\n",
    "\n",
    "**Stop words** are ***extremely common words that would be of little value in our analysis are often excluded from the vocabulary entirely***. Some common **examples** are determiners like the ***, a, an, another,*** but your list of stop words (or <b>stop list</b>) **depends on the context of the problem you're working on. **\n",
    "\n",
    "### 1.6 Testing<a id=\"16\"></a>\n",
    "\n",
    "\n",
    "#### 1.6.1 Cross Validation<a id=\"161\"></a>\n",
    "\n",
    "**Cross validation** is a ***model evaluation method that works by not using the entire data set when training the model***, i.e. some of the data is removed before training begins. ***Once training is completed, the removed data is used to test the performance of the learned model on this data.*** This is important because it prevents your model from over learning (or overfitting) your data.\n",
    "\n",
    "#### 1.6.2 Precision<a id=\"162\"></a>\n",
    "\n",
    "**Precision** is the ***percentage of retrieved instances that are relevant - it measures the exactness of a classifier***. A **higher precision means less false positives**, while **a lower precision means more false positives. **\n",
    "\n",
    "#### 1.6.3 Recall\n",
    "\n",
    "**Recall** is ***the percentage of relevant instances that are retrieved***. **Higher recall means less false negatives, while lower recall means more false negatives**. Improving recall can often decrease precision because it gets increasingly harder to be precise as the sample space increases.\n",
    "\n",
    "#### 1.6.4 F-measure \n",
    "\n",
    "The **f1-score is a measure of a test's accuracy** that considers both the precision and the recall. \n",
    "\n",
    "### 1.7 Logistic Regression<a id=\"17\"></a>\n",
    "\n",
    "**Logistic regression** is a ***generalized linear model commonly used for classifying binary data. Its output is a continuous range of values between 0 and 1***, usually representing the **probability, and its input is some form of discrete predictor. **\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "dc40ed69f92848109875914fbdca96dd5747e406"
   },
   "source": [
    "## 2.0  Information Extraction<a id=\"20\"></a>\n",
    "\n",
    "---\n",
    "Information Extraction is the process of acquiring meaning from text in a computational manner. \n",
    "\n",
    "### 2.1 Data Forms<a id=\"21\"></a>\n",
    "\n",
    "#### 2.1.1 Structured Data\n",
    "\n",
    "Structured Data is when there is a regular and predictable organization of entities and relationships.\n",
    "\n",
    "#### 2.1.2 Unstructured Data\n",
    "\n",
    "Unstructured data, as the name suggests, assumes no organization. This is the case with most written textual data. \n",
    "\n",
    "### 2.2 What is Information Extraction?<a id=\"22\"></a>\n",
    "\n",
    "With that said, information extraction is the means by which you acquire structured data from a given unstructured dataset. There are a number of ways in which this can be done, but generally, information extraction consists of searching for specific types of entities and relationships between those entities. \n",
    "\n",
    "An example is being given the following text, \n",
    "\n",
    "```\n",
    "Martin received a 98% on his math exam, whereas Jacob received a 84%. Eli, who also took the same test, received an 89%. Lastly, Ojas received a 72%.\n",
    "```\n",
    "This is clearly unstructured. It requires reading for any logical relationships to be extracted. Through the use of information extraction techniques, however, we could output structured data such as the following: \n",
    "\n",
    "```\n",
    "Name     Grade\n",
    "Martin   98\n",
    "Jacob    84\n",
    "Eli      89\n",
    "Ojas     72\n",
    "```\n",
    "\n",
    "## 3.0 Chunking<a id=\"30\"></a>\n",
    "\n",
    "---\n",
    "Chunking is used for entity recognition and segments and labels multitoken sequences. This typically involves segmenting multi-token sequences and labeling them with entity types, such as 'person', 'organization', or 'time'. \n",
    "\n",
    "### 3.1 Noun Phrase Chunking<a id=\"31\"></a>\n",
    "\n",
    "Noun Phrase Chunking, or NP-Chunking, is where we search for chunks corresponding to individual noun phrases.\n",
    "\n",
    "We can use nltk, as is the case most of the time, to create a chunk parser. We begin with importing nltk and defining a sentence with its parts-of-speeches tagged (which we covered in the previous tutorial). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_uuid": "845f0699630870e5165f9b2efe60fb01f66fa9ea"
   },
   "outputs": [],
   "source": [
    "import nltk \n",
    "sentence = [(\"the\", \"DT\"), (\"little\", \"JJ\"), (\"yellow\", \"JJ\"), (\"dog\", \"NN\"), (\"barked\", \"VBD\"), (\"at\", \"IN\"), (\"the\", \"DT\"), (\"cat\", \"NN\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f16f8c26dce7c3f9484c704fa2867abc43e759a9"
   },
   "source": [
    "Next, we define the tag pattern of an ***NP chunk.*** A tag pattern is a sequence of ***part-of-speech tags delimited using angle brackets,*** e.g. **`<DT>?<JJ>*<NN>`**. This is how the parse tree for a given sentence is acquired.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_uuid": "d8971eb007e71c71bd0af4e436232ef5b3000ef6"
   },
   "outputs": [],
   "source": [
    "pattern = \"NP: {<DT>?<JJ>*<NN>}\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2d4dd813aa6ec00776f2bb8c295d78a44dd84cff"
   },
   "source": [
    "Finally we create the **chunk parser with the nltk `RegexpParser()` class. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_uuid": "2b602d11cb9911e428d359e033718e88e6810bed"
   },
   "outputs": [],
   "source": [
    "NPChunker = nltk.RegexpParser(pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e35df37525fde4de5e9ad7302c5b281ea8dae3cc"
   },
   "source": [
    "And lastly, we actually **parse** the ***example sentence and display its parse tree.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_uuid": "b05c6bae1941a0e1f881fe13ba78a7dd3829b6a2"
   },
   "outputs": [],
   "source": [
    "result = NPChunker.parse(sentence) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "_uuid": "fa0ce2172e8fbb2d3c7936d7f0a3867a1b0206f8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Tree.draw of Tree('S', [Tree('NP', [('the', 'DT'), ('little', 'JJ'), ('yellow', 'JJ'), ('dog', 'NN')]), ('barked', 'VBD'), ('at', 'IN'), Tree('NP', [('the', 'DT'), ('cat', 'NN')])])>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.draw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1251c2f398d8e1f095fdba2c4c77f85c3cb3aac7"
   },
   "source": [
    "## 4.0 Named Entity Extraction <a id=\"40\"></a>\n",
    "\n",
    "---\n",
    "\n",
    "Named entities are noun phrases that refer to specific types of individuals, such as organizations, people, dates, etc. Therefore, the purpose of a named entity recognition (NER) system is to identify all textual mentions of the named entities.\n",
    "\n",
    "### 4.1 spaCy <a id=\"41\"></a>\n",
    "\n",
    "In the following exercise, we'll build our own named entity recognition system with the Python module `spaCy`, a Python module commonly used for Natural Language Processing in industry. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "_uuid": "4c050287d2c05d50393f1240c54a8546548d282a"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "dcd1463ec927fe1ba122d7367d6f38b3e99b2070"
   },
   "source": [
    "Using `spaCy`, we'll load the built-in **English tokenizer, tagger, parser, NER and word vectors.** We indicate this with the parameter `'en'`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "_uuid": "48a5151e23d19c42ba80338337e2405ac8ee2e2e"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "83c60a02e547cc33a9c199c04b7b3ffcfebbfccb"
   },
   "source": [
    "We need an example to actually process, so below is some text from Columbia's website: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "_uuid": "23c04a277e2ebfce88dfed928ecf09de87ec98d4"
   },
   "outputs": [],
   "source": [
    "review = \"Columbia University was founded in 1754 as King's College by royal charter of King George II of England. It is the oldest institution of higher learning in the state of New York and the fifth oldest in the United States. Controversy preceded the founding of the College, with various groups competing to determine its location and religious affiliation. Advocates of New York City met with success on the first point, while the Anglicans prevailed on the latter. However, all constituencies agreed to commit themselves to principles of religious liberty in establishing the policies of the College. In July 1754, Samuel Johnson held the first classes in a new schoolhouse adjoining Trinity Church, located on what is now lower Broadway in Manhattan. There were eight students in the class. At King's College, the future leaders of colonial society could receive an education designed to 'enlarge the Mind, improve the Understanding, polish the whole Man, and qualify them to support the brightest Characters in all the elevated stations in life.'' One early manifestation of the institution's lofty goals was the establishment in 1767 of the first American medical school to grant the M.D. degree.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c75b5de10ef7337ca4381d2cf742f8ce35f18c33"
   },
   "source": [
    "With this example in mind, we feed it into the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "_uuid": "06f0818578be524f48c7f13490086ce18ae9ae1a"
   },
   "outputs": [],
   "source": [
    "doc = nlp(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "720afb6762242538a57420e6acbad559f25d12ed"
   },
   "source": [
    "Going along ***the process of named entity extraction, we begin by segmenting the text, i.e. splitting it into a list of sentences. ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "_uuid": "acd2966bbf1423dac505e69aeea40e715c402456"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 9 sentences found.\n"
     ]
    }
   ],
   "source": [
    "sentences = [sentence.orth_ for sentence in doc.sents] # list of sentences\n",
    "print(\"There were {} sentences found.\".format(len(sentences)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a02a3685e9e718d47325c67aa77ad336720bbe2c"
   },
   "source": [
    "Now, we go a step further, and **count the number of nounphrases** by **taking advantage of chunk properties.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "_uuid": "dad908964764c54d94c0c22b111c8777f0b95b25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 54 noun phrases found.\n"
     ]
    }
   ],
   "source": [
    "nounphrases = [[np.orth_, np.root.head.orth_] for np in doc.noun_chunks]\n",
    "print(\"There were {} noun phrases found.\".format(len(nounphrases)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a1df8be6f0732957e6b04eb4f79b232923ec5c44"
   },
   "source": [
    "#### Lastly, we achieve our final goal: entity extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "_uuid": "e6eec36f0fba1d9125e5c4716b2bcc3ab34b84a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 28 entities found\n"
     ]
    }
   ],
   "source": [
    "entities = list(doc.ents) # converts entities into a list\n",
    "print(\"There were {} entities found\".format(len(entities)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "12ceb01dbe0e298128a28294c5a0a1a40e230a16"
   },
   "source": [
    "#### So now, we can turn this into a DataFrame for better visualization: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "_uuid": "4d89e34538de97895b61e0c98b26fdc340bce7a4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Columbia University</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>King's College</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>George II</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>College</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>College</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Samuel Johnson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Trinity Church</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>King's College</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Characters</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     0\n",
       "0  Columbia University\n",
       "1       King's College\n",
       "2            George II\n",
       "3              College\n",
       "4              College\n",
       "5       Samuel Johnson\n",
       "6       Trinity Church\n",
       "7       King's College\n",
       "8           Characters"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orgs_and_people = [entity.orth_ for entity in entities if entity.label_ in ['ORG','PERSON']]\n",
    "pd.DataFrame(orgs_and_people)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1a5054f8c158c20f87bb66f865f882fe92043009"
   },
   "source": [
    "#### In summary, named entity extraction typically follows the process of sentence segmentation, noun phrase chunking, and, finally, entity extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d3c4e36b574cfa76dcd279972057fb4dfe78990e"
   },
   "source": [
    "\n",
    "### 4.2 nltk <a id=\"42\"></a>\n",
    "\n",
    "Next, we'll work through a **similar example as before, this time using the nltk module to extract the named entities through the use of chunk parsing.** As always, we begin by importing our needed modules and example:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "_uuid": "3d6dc799360eeac87405ec4a38008f03a2ea5bef"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "content = \"Starbucks has not been doing well lately\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3a9d69b42b4441dc924cce79024512aa049c7135"
   },
   "source": [
    "#### Then, as always, we tokenize the sentence and follow up with parts-of-speech tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "_uuid": "3d3b10b3c1bfff15ed7c1e655460de0fd1c179fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Starbucks', 'NNP'), ('has', 'VBZ'), ('not', 'RB'), ('been', 'VBN'), ('doing', 'VBG'), ('well', 'RB'), ('lately', 'RB')]\n"
     ]
    }
   ],
   "source": [
    "tokenized = nltk.word_tokenize(content)\n",
    "tagged = nltk.pos_tag(tokenized)\n",
    "print(tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c17b29fe6577b1532b0317ee70680d2c2610f4a7"
   },
   "source": [
    "#### So we take this POS tagged sentence and feed it to the `nltk.ne_chunk()` method. This method returns a nested Tree object, so we display the content with namedEnt.draw(). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "_uuid": "20f8f0119fb1922e741b78c38ebb23fb5275cdb9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Tree.draw of Tree('S', [Tree('PERSON', [('Starbucks', 'NNP')]), ('has', 'VBZ'), ('not', 'RB'), ('been', 'VBN'), ('doing', 'VBG'), ('well', 'RB'), ('lately', 'RB')])>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "namedEnt = nltk.ne_chunk(tagged)\n",
    "namedEnt.draw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2c8ca325450412d98075eab56de12475cc1c5389"
   },
   "source": [
    "Now, if you wanted to simply get the named entities from the namedEnt object we created, how do you think you would go about doing so?\n",
    "\n",
    "## 5.0 Relation Extraction <a id=\"50\"></a>\n",
    "\n",
    "---\n",
    "\n",
    "Once we have identified named entities in a text, we then want to analyze for the relations that exist between them. This can be performed using either rule-based systems, which typically look for specific patterns in the text that connect entities and the intervening words, or using machine learning systems that typically attempt to learn such patterns automatically from a training corpus.\n",
    "\n",
    "### 5.1 Rule-Based Systems<a id=\"51\"></a>\n",
    "\n",
    "In the rule-based systems approach, we look for all triples of the form (X, a, Y), where X and Y are named entities and a is the string of words that indicates the relationship between X and Y. Using regular expressions, we can pull out those instances of a that express the relation that we are looking for. \n",
    "\n",
    "In the following code, we search for strings that contain the word \"in\". The special regular expression `(?!\\b.+ing\\b)` allows us to disregard strings such as `success in supervising the transition of`, where \"in\" is followed by a gerund. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "_uuid": "45493934d662e54ae163a3c37358feedf495a01b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ORG: 'WHYY'] 'in' [LOC: 'Philadelphia']\n",
      "[ORG: 'McGlashan &AMP; Sarrail'] 'firm in' [LOC: 'San Mateo']\n",
      "[ORG: 'Freedom Forum'] 'in' [LOC: 'Arlington']\n",
      "[ORG: 'Brookings Institution'] ', the research group in' [LOC: 'Washington']\n",
      "[ORG: 'Idealab'] ', a self-described business incubator based in' [LOC: 'Los Angeles']\n",
      "[ORG: 'Open Text'] ', based in' [LOC: 'Waterloo']\n",
      "[ORG: 'WGBH'] 'in' [LOC: 'Boston']\n",
      "[ORG: 'Bastille Opera'] 'in' [LOC: 'Paris']\n",
      "[ORG: 'Omnicom'] 'in' [LOC: 'New York']\n",
      "[ORG: 'DDB Needham'] 'in' [LOC: 'New York']\n",
      "[ORG: 'Kaplan Thaler Group'] 'in' [LOC: 'New York']\n",
      "[ORG: 'BBDO South'] 'in' [LOC: 'Atlanta']\n",
      "[ORG: 'Georgia-Pacific'] 'in' [LOC: 'Atlanta']\n"
     ]
    }
   ],
   "source": [
    "IN = re.compile(r'.*\\bin\\b(?!\\b.+ing)')\n",
    "for doc in nltk.corpus.ieer.parsed_docs('NYT_19980315'):\n",
    "    for rel in nltk.sem.relextract.extract_rels('ORG', 'LOC', doc,corpus='ieer', pattern = IN):\n",
    "        print (nltk.sem.relextract.rtuple(rel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b6d52ea5ef7cf6dfcd85e6b173d47321114bd57d"
   },
   "source": [
    "Note that the X and Y named entitities types all match with one another! Object type matching is an important and required part of this process. \n",
    "\n",
    "### 5.2 Machine Learning <a id=\"52\"> </a>\n",
    "\n",
    "We won't be going through an example of a machine learning based entity extraction algorithm, but it's important to note the different machine learning algorithms that can be implemented to accomplish this task of relation extraction. \n",
    "\n",
    "Most simply, Logistic Regression can be used to classify the objects that relate to one another. But additionally, algorithms like Suport Vector Machines and Random Forest could also accomplish the job. Which algorithm you ultimately choose depends on which outperforms in terms of speed and accuracy.\n",
    "\n",
    "In summary, it's important to note that while these algorithms will likely have high accurate rates, labeling thousands of relations (and entities!) is incredibly expensive.   \n",
    "\n",
    "\n",
    "## 6.0 Sentiment Analysis<a id=\"60\"></a>\n",
    "\n",
    "---\n",
    "\n",
    "As we saw in the previous tutorial, sentiment analysis refers to the use of text analysis and statistical learning to identify and extract subjective information in textual data. For our last exercise in this tutorial, we'll introduce and use linear models in the context of a sentiment analysis problem.\n",
    "\n",
    "### 6.1 Loading the Data<a id=\"61\"></a>\n",
    "\n",
    "First, we begin by loading the data. Since we'll be using data available online, we'll use the urllib module to avoid having to manually download any data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "_uuid": "8b9a72393f72540c93e3dcd2ece56534a9fa1337"
   },
   "outputs": [],
   "source": [
    "import urllib.request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2c0e3a59eb9e522507cd7a13ddb1bfc3f2108c19"
   },
   "source": [
    "So then we'll define the ***test and training data URLs to variables, as well as filenames for each of those datasets.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "_uuid": "c8cc2a885ca34fbb2e06b6d43cedafd9b7ff7c03"
   },
   "outputs": [],
   "source": [
    "test_file = '../input/sentimental-analysis-nlp/test_data.csv'\n",
    "train_file = '../input/sentimental-analysis-nlp/train_data.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "214c42a1de6bfda71b13cd9a751532faad3fd292"
   },
   "source": [
    "Using the **links and filenames** from above, we'll officially download the data using the **urlib.request.urlretrieve** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "_uuid": "535b24c7588dfea483d58a29f96e3cd9adbcd45e"
   },
   "outputs": [],
   "source": [
    "# test_data_f = urllib.request.urlretrieve(test_file)\n",
    "# train_data_f = urllib.request.urlretrieve(train_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c51947a9095d5e1ebf6166f4e5564e04118371ce"
   },
   "source": [
    "#### Now that we've downloaded our datasets, we can load them into pandas dataframes. First for the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "_uuid": "94955033515f36e301f3d7bd298c189727b5abee"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_data_df = pd.read_csv(test_file, header=None, delimiter=\"\\t\", quoting=3)\n",
    "test_data_df.columns = [\"Text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b581ac53227582e6940d6b2aba16b78cfbe5f4b0"
   },
   "source": [
    "### Next for training data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "_uuid": "e36626ab1468f21aad6c53371a072f8dba57ef6e"
   },
   "outputs": [],
   "source": [
    "train_data_df = pd.read_csv(train_file, header=None, delimiter=\"\\t\", quoting=3)\n",
    "train_data_df.columns = [\"Sentiment\",\"Text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0b9ebbca96b50c74e38e9153531fd464f7945acf"
   },
   "source": [
    "#### Just to see how the dataframe looks, let's call the .head() method on both dataframes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "_uuid": "8885915d1de8dead70e383b88aa345752b6d993e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\" I don't care what anyone says, I like Hillar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>have an awesome time at purdue!..</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yep, I'm still in London, which is pretty awes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Have to say, I hate Paris Hilton's behavior bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i will love the lakers.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text\n",
       "0  \" I don't care what anyone says, I like Hillar...\n",
       "1                  have an awesome time at purdue!..\n",
       "2  Yep, I'm still in London, which is pretty awes...\n",
       "3  Have to say, I hate Paris Hilton's behavior bu...\n",
       "4                            i will love the lakers."
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "_uuid": "f937d6afde65f02fbae5fdb65746986c806cce54"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The Da Vinci Code book is just awesome.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>this was the first clive cussler i've ever rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>i liked the Da Vinci Code a lot.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>i liked the Da Vinci Code a lot.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>I liked the Da Vinci Code but it ultimatly did...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment                                               Text\n",
       "0          1            The Da Vinci Code book is just awesome.\n",
       "1          1  this was the first clive cussler i've ever rea...\n",
       "2          1                   i liked the Da Vinci Code a lot.\n",
       "3          1                   i liked the Da Vinci Code a lot.\n",
       "4          1  I liked the Da Vinci Code but it ultimatly did..."
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "53dd214610a1868c6ce4afe3934cf156dba2ee26"
   },
   "source": [
    "### 6.2 Preparing the Data<a id=\"62\"></a>\n",
    "\n",
    "To implement our bag-of-words linear classifier, we need our data in a format that allows us to feed it in to the classifer. Using sklearn.feature_extraction.text.CountVectorizer in the Python scikit learn module, we can convert the text documents to a matrix of token counts. So first, we import all the needed modules: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "_uuid": "c9463df3339e17018773872987211b7867425c03"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer        \n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a85becf625271a8ec1ccdd5d3c5a98728805ca10"
   },
   "source": [
    "We need to **remove punctuations, lowercase, remove stop words, and stem words.** All these steps can be **directly performed by CountVectorizer** if we pass the right parameter values. We can do this as follows. \n",
    "\n",
    "We first create a **stemmer, using the Porter Stemmer implementation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "_uuid": "12951aaf20d2856a87afd18342c4a6bee452028c"
   },
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = [stemmer.stem(item) for item in tokens]\n",
    "    return(stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4c45276fc9b8b172c7e22d9383511b76d2141049"
   },
   "source": [
    "#### Here, we have our tokenizer, which removes non-letters and stems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "_uuid": "e19cf60022a85b6f332daeca5a77261a8f833e18"
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return(stems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ac90436d57744aae754ceaaa8efd35e17f62e6b8"
   },
   "source": [
    "#### Here we init the vectoriser with the CountVectorizer class, making sure to pass our tokenizer and stemmers as parameters, remove stop words, and lowercase all characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "_uuid": "a2316944b9aa4c2061aab96d1bd347c5817bfa7e"
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(\n",
    "analyzer = 'word',\n",
    "tokenizer = tokenize,\n",
    "lowercase = True,\n",
    "stop_words = 'english',\n",
    "max_features = 85\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "93b39010a3548170ea77ec001651dd96576ffeb7"
   },
   "source": [
    "Next, we use the ***fit_transform()*** method to **transform our corpus data into feature vectors.** Since the input needed is a **list of strings, we concatenate all of our training and test data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "_uuid": "95d3f7c8a75f0193464d456d09e07229f2ec0a9d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "features = vectorizer.fit_transform(\n",
    "train_data_df.Text.tolist() + test_data_df.Text.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5980a923a310bbc7cea9175d24bb65592a40bede"
   },
   "source": [
    "Here, we're simply converting the features to an array for easier use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "_uuid": "706abda9a085d249cf8ed8fcc45b67a071a3d7eb"
   },
   "outputs": [],
   "source": [
    "features_nd = features.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d5da6e2110d9e04a269aea448dee8fe28b004df1"
   },
   "source": [
    "### 6.3 Linear Classifier<a id=\"63\"></a>\n",
    "Finally, we begin building our classifier. Earlier we learned what a bag-of-words model. Here, we'll be using a similar model, but with some modifications. To refresh your mind, this kind of model simplifies text to a multi-set of terms frequencies.\n",
    "\n",
    "So first we'll split our training data to get an evaluation set. As we mentioned before, we'll use cross validation to split the data. sklearn has a built-in method that will do this for us. All we need to do is provide the data and assign a training percentage (in this case, 75%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "_uuid": "34b0ae63ffbec89aa26e5f90be83a9e836b56c4f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test  = train_test_split(features_nd[0:len(train_data_df)], train_data_df.Sentiment,train_size=0.85, random_state=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d8bf3a8e266bdc35d10c559f950bfc742f43a2ca"
   },
   "source": [
    "Now we're ready to **train our classifier.** We'll be using **Logistic Regression to model this data.** Once again, **sklearn has a built-in model for you to use, so we begin by importing the needed modules and calling the class.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "_uuid": "ba8fbf2d8e3ac3f82d8501acb3dd6edab0bdf92c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 0.984725 (0.004057)\n",
      "LDA: 0.984559 (0.003409)\n",
      "KNN: 0.983562 (0.006058)\n",
      "CART: 0.986219 (0.002787)\n",
      "NB: 0.938905 (0.024669)\n",
      "SVM: 0.982399 (0.004998)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "\n",
    "log_model = LogisticRegression()\n",
    "\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression()))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC()))\n",
    "seed = 7\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "scoring = 'accuracy'\n",
    "for name, model in models:\n",
    "    print(name)\n",
    "    kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "    cv_results = model_selection.cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n",
    "    model.fit(X=X_train, y=y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    prediction = pd.DataFrame(predictions, columns=['predictions']).to_csv(name + ' prediction.csv', index = False)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    print(classification_report(y_test, predictions))\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)\n",
    "    \n",
    "# boxplot algorithm comparison\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2e670259c444b2805c9bc49c695cde5d40d143d7"
   },
   "source": [
    "And as always, we need actually do the training, so we call the **`.fit()`** method on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "_uuid": "9847a615d0d89aef61e97e565a88d459fba29fbb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# log_model = log_model.fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ef90ee50d2b67dbe03530e7bbc7e23319363c3e2"
   },
   "source": [
    "Now we use the classifier to label the evaluation set we created earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "_uuid": "c6784756bef87afbaba4ea1eeb25a04004180231"
   },
   "outputs": [],
   "source": [
    "# y_pred = log_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "_uuid": "eb5556f7fe6ec52e7e83bbc2eb04efec674f076b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, ..., 0, 1, 0])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6f6dba1c205e8a5b972fc24915dd24bce3f5a84e"
   },
   "source": [
    "### 6.4 Accuracy <a id=\"64\"></a>\n",
    "\n",
    "In sklearn, there is a function called sklearn.metrics.classification_report which calculates several types of predictive scores on a classification model. So here we check out how exactly our model is performing:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "_uuid": "41fa853456c3108ccf606571fd101c527234da88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       467\n",
      "           1       0.99      0.98      0.99       596\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1063\n",
      "   macro avg       0.98      0.99      0.98      1063\n",
      "weighted avg       0.98      0.98      0.98      1063\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d1ccbb3e9590910d18c0c8287b5dcf983b1149c2"
   },
   "source": [
    "where precision, recall, and f1-score are the accuracy values discussed in the section 1.6. Support is the number of occurrences of each class in y_true and x_true.\n",
    "\n",
    "\n",
    "### 6.5 Retraining <a id=\"65\"></a>\n",
    "\n",
    "Finally, we can re-train our model with all the training data and use it for sentiment classification with the original unlabeled test set. \n",
    "\n",
    "So we repeat the process from earlier, this time with different data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "_uuid": "cce0ff8c49a00b8c0f67a8bf273df5c43211bd67"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "log_model = LogisticRegression()\n",
    "log_model = log_model.fit(X=features_nd[0:len(train_data_df)], y=train_data_df.Sentiment)\n",
    "test_pred = log_model.predict(features_nd[len(train_data_df):])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2c26bacf7d87cb938094f03d19c933c84c413c3d"
   },
   "source": [
    "So again, we can see what the predictions look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "_uuid": "65f147e36ae6a3a4dde08ee5b5bae75cc7dff8e6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 0])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cec9152ac7678fb724a74bfe6d9a5d74b94aa373"
   },
   "source": [
    "And lastly, let's actually look at our predictions! Using the random module to select a random sliver of the data we predicted on, we'll print the results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "_uuid": "d2937df80a100543867a19072bf18b22e2b98fe5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 i try to enjoy my time here in san francisco.......\n",
      "1 Three days at Purdue with three awesome people.\n",
      "0 It was really ironic that he spent the first part of class talking about his own professor at Harvard who was a pompous arrogant ass.\n",
      "0 The Lakers can suck me...\n",
      "1 I miss my UCLA buds a lot, especially my BW friends.\n",
      "0 I hate Paris Hilton Paris got arrested last night for a DUI, spent 15 minutes in jail, and in the morning called Ryan Seacrest to \" set the record straight.\n",
      "0 i hate san francisco, so i don't blame the tour organizers for skipping it, but man, that means i cna't hear you on the radio.\n",
      "0 I was rejected by the stupid San Francisco literary agency that I sent my manuscript to.\n",
      "1 buy quite a few food to back to notts to eat la, aiiii, notts only hv 1 chinese shop in town ja, so shit, london is so GREAT!!..\n",
      "1 I'm not crazy about HK either, but Shanghai is sounding awesome.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "spl = random.sample(range(len(test_pred)), 10)\n",
    "for text, sentiment in zip(test_data_df.Text[spl], test_pred[spl]):\n",
    "    print(sentiment, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0d19762b93b268770b531fed93bc688dfe8a9880"
   },
   "source": [
    "### Thanks for reading...Happy learning!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
